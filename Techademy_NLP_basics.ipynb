{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Techademy_NLP_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYXouMeL+oYWB8hDPr5EyV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anomishra/Techademy_Artificial_intelligence/blob/master/Techademy_NLP_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jwzJ20D8eRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install nltk\n",
        "# if you are getting error for nltk, then install nltk using above script"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGI6blB_x9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbhrfA8VBRcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Error\n",
        "---------------------------------------------------------------------------\n",
        "LookupError                               Traceback (most recent call last)\n",
        "<ipython-input-4-65208e546d17> in <module>()\n",
        "      1 from nltk.tokenize import word_tokenize\n",
        "      2 \n",
        "----> 3 tokens = word_tokenize(sentence)\n",
        "\n",
        "4 frames\n",
        "/usr/local/lib/python3.6/dist-packages/nltk/data.py in find(resource_name, paths)\n",
        "    671     sep = '*' * 70\n",
        "    672     resource_not_found = '\\n%s\\n%s\\n%s\\n' % (sep, msg, sep)\n",
        "--> 673     raise LookupError(resource_not_found)\n",
        "    674 \n",
        "    675 \n",
        "\n",
        "LookupError: \n",
        "**********************************************************************\n",
        "  Resource punkt not found.\n",
        "  Please use the NLTK Downloader to obtain the resource:\n",
        "\n",
        "  >>> import nltk\n",
        "  >>> nltk.download('punkt')\n",
        "  \n",
        "  Searched in:\n",
        "    - '/root/nltk_data'\n",
        "    - '/usr/share/nltk_data'\n",
        "    - '/usr/local/share/nltk_data'\n",
        "    - '/usr/lib/nltk_data'\n",
        "    - '/usr/local/lib/nltk_data'\n",
        "    - '/usr/nltk_data'\n",
        "    - '/usr/lib/nltk_data'\n",
        "    - ''\n",
        "***********************************************"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58MaUxwRBMHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0-aD3BAFJn",
        "colab_type": "text"
      },
      "source": [
        "##Tokenization##\n",
        "1. Process of breaking the sentence into words </br>\n",
        "2. Structures the input senstence; data representation </br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2UmeAVeAoYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence  = \"We all are in the Techademy class. And this class belongs to artificial intelligence. We are learning artificial intelligence including machine learning , #NLP, #deep learning, #computer vision and various other applications of artificial intelligence. This course is useful.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teKWrD3IA-Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "210MamVtBF5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRZF6vP9BhxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Counting the word length\n",
        "len(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q88rH3GQBopH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# counting the word count in the sentence\n",
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()\n",
        "\n",
        "for word in tokens:\n",
        "  fdist[word.lower()]+= 1       #tokens are convereted in lowercase\n",
        "\n",
        "fdist\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfYzjO_gCQTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# most common used chracters in the sentence\n",
        "\n",
        "fdist_top3 = fdist.most_common(3)\n",
        "fdist_top3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW--9VZ-CzUu",
        "colab_type": "text"
      },
      "source": [
        "**Tokenisation types** </br>\n",
        "Ngram: tokens of N consecutive words </br>\n",
        "Bigram: tokens with 2 consecutive words. </br>\n",
        "Trigram: tokens with 3 consecutive words </br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHhFRq3yCthy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgPVf2oDp3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_bigrams = list(bigrams(tokens))\n",
        "sent_bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG9vhwj3D8o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_bigrams = list(bigrams(tokens))\n",
        "sent_bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISQW8NLaEBg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ngrams(tokens,N)       Fill the value of N\n",
        "sent_ngrams = list(ngrams(tokens,4))\n",
        "sent_ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsdGAcjdEXO3",
        "colab_type": "text"
      },
      "source": [
        "##Stemming##\n",
        "Transform the words into its root form </br>\n",
        "having ---> have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgJ1HzviEmK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLM0lZq6E07p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pst.stem('having') # root word for having "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOISWQ7sE_y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stem_words= ['give','giving','given','gave']\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + pst.stem(words))       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKIlowtsGbyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stem_words_1= ['go', 'went', 'gone']\n",
        "for words in stem_words_1:\n",
        "  print(words + \":\" + pst.stem(words))     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuqBEgcHFmh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another form of stemmer\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + lst.stem(words))  \n",
        "\n",
        "# bit more complex  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daflACQ0LEhh",
        "colab_type": "text"
      },
      "source": [
        "##POS: Parts of Speech##\n",
        "1. Process grammar of words\n",
        "2. Helpful in the meaning for word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLbfNRUMfY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('tagsets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc1G5Me4Mcbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can see different tags for the English Grammer using this script\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPGZCEXmMvNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import pos_tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiCKYu38NAF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-9-ajThM3Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in tokens: \n",
        "  print(pos_tag([token]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50x6fU7CNElf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b2a64c15-d625-47dc-91a6-30a1545ca19c"
      },
      "source": [
        "new_sent = \"Dilanga is teaching AI class\"\n",
        "new_sent = word_tokenize(new_sent)\n",
        "for token in new_sent: \n",
        "  print(pos_tag([token]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Dilanga', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('teaching', 'VBG')]\n",
            "[('AI', 'NN')]\n",
            "[('class', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7AKd-pJF_xU",
        "colab_type": "text"
      },
      "source": [
        "##Lemmatization##\n",
        "1. Process morphological analysis of the word </br>\n",
        "2. Similar to stemming, maps words to one common root </br>\n",
        "3. Ouput is a proper word, hence required a detailed dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkBdStxIGm17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import wordnet # detailed dictionary\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lemma = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnPleqhIIS78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5578d82a-4dbe-477e-ec18-3becb05c0eca"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tisvo3JWH94R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86791614-31bd-411c-9807-f8fc8f08b64f"
      },
      "source": [
        "word_lemma.lemmatize('stemming')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stemming'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4xt9r8ONvSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f93c5c55-0cfd-4e04-ef4e-e0e370844686"
      },
      "source": [
        "word_lemma.lemmatize('stemming', 'v')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stem'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y2HSnjEIYQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b04c37da-2c3a-45bf-f832-36904414153f"
      },
      "source": [
        "stem_words= ['give','giving','given','gave']\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + word_lemma.lemmatize(words))  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:giving\n",
            "given:given\n",
            "gave:gave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlpcWJUsIjMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "568e88d1-fc94-426e-9077-ae2f11b03062"
      },
      "source": [
        "stem_words= ['give','giving','given','gave']\n",
        "for words in stem_words:\n",
        "  print(words + \":\" + word_lemma.lemmatize(words, 'v'))  "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:give\n",
            "given:give\n",
            "gave:give\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSd1ZTemOSR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5363c698-968a-4f9d-cafe-2655303d17f6"
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "  \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
        "  \n",
        "# a denotes adjective in \"pos\" \n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWzTj7DBIqBx",
        "colab_type": "text"
      },
      "source": [
        "##StopWords##\n",
        "1. I, am, the, if.............</br>\n",
        "2. Not helpful in the processing of the language </br>\n",
        "2. Only helful in creating the sentence </br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjSpozDkI8DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ugMxuSLJGy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amKxnUIwI_r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOPMZn2OJR22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ce28538-06dc-4e01-8dfa-9947b0cf2753"
      },
      "source": [
        "len(stopwords.words('english'))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S_ejBP-Jepc",
        "colab_type": "text"
      },
      "source": [
        "####Regular Expression ####\n",
        "1. Sequence of characters that define search pattern </br>\n",
        "2. We use these for any desired ouput extraction say words with numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxDyTkW7J08q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM0-1KM_KCmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_punct = []\n",
        "\n",
        "for words in tokens:\n",
        "    word = punctuation.sub(\"\", words)\n",
        "    if len(word)>0:\n",
        "      post_punct.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xMMv6WIKUTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_punct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhwBRDqlKthn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cca1acf9-8c26-4cb5-fb81-b627327ed980"
      },
      "source": [
        "\n",
        "len(post_punct)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqwkkJVvKyuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d7b416b-bcad-4382-b660-0186de0b3936"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex2-nmK9LTL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Alternative approach\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  \n",
        "print(word_tokens) \n",
        "print(\"length of original sentence: \", len(word_tokens))\n",
        "print(filtered_sentence)\n",
        "print(\"length of filtered sentence: \", len(filtered_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmUJRMvyLwyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_sentence_alternate = [w for w in word_tokens if not w in stop_words] \n",
        "print(filtered_sentence_alternate)\n",
        "print(\"length of altenate filtered sentence: \", len(filtered_sentence_alternate))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwJ7ImiLOmV9",
        "colab_type": "text"
      },
      "source": [
        "## Name Entity Recognition ##\n",
        "\n",
        "1. Noun phrase Identification \n",
        "2. realted to POS classification; POS tags is required\n",
        "3. Ex: In Techademy, Dilanga teach AI at ZOOM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldM_CxVsPHY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ne_chunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C6jJcmsPNd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_1 = \"The US President stays in White House\"\n",
        "sent_1_token = word_tokenize(sent_1)\n",
        "sent_1_tags = nltk.pos_tag(sent_1_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GoJm7TxPpEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFs8b6ZKPsKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "43a5f0ef-c2ca-4e74-ff45-b8167aec48ce"
      },
      "source": [
        "nltk.download('words')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLdxse6lPc5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "a72dc7fc-6301-4aa4-8f2d-8ef654f77b00"
      },
      "source": [
        "NER = ne_chunk(sent_1_tags)\n",
        "print(NER)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION US/NNP)\n",
            "  President/NNP\n",
            "  stays/VBZ\n",
            "  in/IN\n",
            "  (FACILITY White/NNP House/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}